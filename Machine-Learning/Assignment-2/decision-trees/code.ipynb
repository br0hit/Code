{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "### Name:   Bannuru Rohit Kumar Reddy\n",
    "### Roll Number:    21CS30011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries here\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../dataset/decision-tree.csv')\n",
    "print(df.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understanding the type  of Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the data before we proceed further : \n",
    "\n",
    "print(df.dtypes)\n",
    "print(df.describe())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into training,test and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are no NULL values, let us proceed with normalizing the dataset and then split it in the 80 - 20 Fashion \n",
    "\n",
    "# split the dataset into train and test and validation here\n",
    "train_df, temp_data_df = train_test_split(df, test_size=0.5, random_state=39)\n",
    "validation_df, test_df = train_test_split(temp_data_df, test_size=0.4, random_state=39)\n",
    "\n",
    "# Convert DataFrame to numpy arrays\n",
    "train_data = train_df.to_numpy()\n",
    "test_data = test_df.to_numpy()\n",
    "validation_data = validation_df.to_numpy()\n",
    "\n",
    "# Split data into input and output features \n",
    "X_train = train_data[:, :-1]   \n",
    "X_test = test_data[:, :-1] \n",
    "X_validation = validation_data[:, :-1]                 \n",
    "y_train = train_data[:, -1]                     \n",
    "y_test = test_data[:, -1]\n",
    "y_validation = validation_data[:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing ID3 Decision Tree\n",
    "\n",
    "##### Steps Involved : \n",
    "1. Calculating Enotrpy\n",
    "2. Find the information gain based on the entropy\n",
    "3. Find the best attribute based on the information gained function, This must return the best attribute along with the position at which the attribute must be split to get the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    total_entropy = 0\n",
    "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "    for count in class_counts:\n",
    "        probability = count / len(y)\n",
    "        total_entropy -= probability * np.log2(probability)\n",
    "    return total_entropy\n",
    "\n",
    "def info_gain(X, y, attribute_index, threshold):\n",
    "    initial_entropy = entropy(y)\n",
    "    entropy_after_split = 0\n",
    "    y_left = []\n",
    "    y_right = []\n",
    "    for i in range(len(y)):\n",
    "        if X[i][attribute_index] <= threshold:\n",
    "            y_left.append(y[i])\n",
    "        else:\n",
    "            y_right.append(y[i])\n",
    "    entropy_after_split = (len(y_left) / len(y)) * entropy(y_left) + (len(y_right) / len(y)) * entropy(y_right)\n",
    "    return initial_entropy - entropy_after_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best attribute and threshold value\n",
    "def best_attribute_threshold(X, y):\n",
    "    \n",
    "    best_attribute_index = 0\n",
    "    best_threshold = 0\n",
    "    max_info_gain = 0\n",
    "    for i in range(X.shape[1]):\n",
    "        for j in range(X.shape[0]):\n",
    "            info_gain_val = info_gain(X, y, i, X[j][i])\n",
    "            if info_gain_val > max_info_gain:\n",
    "                max_info_gain = info_gain_val\n",
    "                best_attribute_index = i\n",
    "                best_threshold = X[j][i]\n",
    "    return (best_attribute_index, best_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the base of the tree in terms of  Nodes \n",
    "1. Define Node\n",
    "2. Build Tree based on the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node class for the decision tree\n",
    "class Node:\n",
    "    def __init__(self, attribute_index=None, threshold=None, left=None, right=None, label=None):\n",
    "        self.attribute_index = attribute_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.label = label\n",
    "    \n",
    "    def is_leaf_node(self):\n",
    "        return self.label is not None\n",
    "\n",
    "\n",
    "# Building the decision tree using the ID3 algorithm\n",
    "# tree having  min_size as stopping criteria\n",
    "\n",
    "def build_tree(X, y, min_size):\n",
    "   \n",
    "    if len(y) <= min_size:\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        return Node(label=unique[np.argmax(counts)])\n",
    "    best_attribute_index, best_threshold = best_attribute_threshold(X, y)\n",
    "    y_left = []\n",
    "    y_right = []\n",
    "    X_left = []\n",
    "    X_right = []\n",
    "    for i in range(len(y)):\n",
    "        if X[i][best_attribute_index] <= best_threshold:\n",
    "            y_left.append(y[i])\n",
    "            X_left.append(X[i])\n",
    "        else:\n",
    "            y_right.append(y[i])\n",
    "            X_right.append(X[i])\n",
    "    if len(y_left) == 0 or len(y_right) == 0:\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        return Node(label=unique[np.argmax(counts)])\n",
    "    left = build_tree(np.array(X_left), np.array(y_left), min_size)\n",
    "    right = build_tree(np.array(X_right), np.array(y_right), min_size)\n",
    "    return Node(best_attribute_index, best_threshold, left, right)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utility Functions : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, data_point):\n",
    "    if node.is_leaf_node():\n",
    "        return node.label\n",
    "    if data_point[node.attribute_index] <= node.threshold:\n",
    "        return predict(node.left, data_point)\n",
    "    else:\n",
    "        return predict(node.right, data_point)\n",
    "\n",
    "def predict_labels(root, X):\n",
    "    y_pred = []\n",
    "    for i in range(len(X)):\n",
    "        y_pred.append(predict(root, X[i]))\n",
    "    return np.array(y_pred)\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    correct = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            correct += 1\n",
    "    return correct/len(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_decision_tree(node, feature_names, graph=None):\n",
    "    if graph is None:\n",
    "        graph = Digraph(format='png')  # You can change the format if you prefer a different image format\n",
    "    \n",
    "    if node.is_leaf():\n",
    "        graph.node(str(id(node)), label=str(node.output_label))\n",
    "    else:\n",
    "        feature_name = feature_names[node.split_attribute]\n",
    "        graph.node(str(id(node)), label=f\"{feature_name}\\nThreshold {node.split_threshold}\")\n",
    "        if node.left:\n",
    "            visualize_decision_tree(node.left, feature_names, graph)\n",
    "            graph.edge(str(id(node)), str(id(node.left)), label='correct')\n",
    "        if node.right:\n",
    "            visualize_decision_tree(node.right, feature_names, graph)\n",
    "            graph.edge(str(id(node)), str(id(node.right)), label='incorrect')\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning:\n",
    "def prune_decision_tree(node, validation_data, validation_labels):\n",
    "    if node.is_leaf():\n",
    "        return node\n",
    "\n",
    "    if node.left.is_leaf() and node.right.is_leaf():\n",
    "        # Calculate accuracy before pruning\n",
    "        predicted_labels_before_pruning = predict_labels(node, validation_data)\n",
    "        accuracy_before = compute_accuracy(predicted_labels_before_pruning, validation_labels)\n",
    "\n",
    "        # Prune the node by setting it as a leaf with the majority class\n",
    "        unique_labels, label_counts = np.unique(validation_labels, return_counts=True)\n",
    "        most_common_label = unique_labels[np.argmax(label_counts)]\n",
    "        node.set_as_leaf(most_common_label)\n",
    "\n",
    "        # Calculate accuracy after pruning\n",
    "        predicted_labels_after_pruning = predict_labels(node, validation_data)\n",
    "        accuracy_after = compute_accuracy(predicted_labels_after_pruning, validation_labels)\n",
    "\n",
    "        # If accuracy doesn't improve, revert the pruning\n",
    "        if accuracy_after < accuracy_before:\n",
    "            node.revert_pruning()\n",
    "        \n",
    "        return node\n",
    "\n",
    "    # Recursively prune the left and right subtrees\n",
    "    node.left = prune_decision_tree(node.left, validation_data, validation_labels)\n",
    "    node.right = prune_decision_tree(node.right, validation_data, validation_labels)\n",
    "\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the main tree and the pruned tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_decision_tree = build_tree(X_train, y_train, 10)\n",
    "\n",
    "# Names of the features \n",
    "feature_names = list(df.columns[:-1])\n",
    "\n",
    "\n",
    "graph = visualize_tree(base_decision_tree, feature_names)\n",
    "graph.render('decision_tree', view=True)\n",
    "y_pred = predict_labels(base_decision_tree, X_test)\n",
    "test_accuracy = accuracy(y_pred, y_test)\n",
    "\n",
    "\n",
    "# Prune the tree and repeat the same thing \n",
    "pruned_tree = reduced_error_pruning(base_decision_tree, X_validation, y_validation)\n",
    "graph = visualize_tree(pruned_tree, feature_names)\n",
    "graph.render('pruned_decision_tree', view=True)\n",
    "y_pred = predict_labels(pruned_tree, X_test)\n",
    "test_accuracy = accuracy(y_pred, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test accuracy pruning: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
