{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "### Name: Bannuru Rohit Kumar Reddy\n",
    "### Roll Number: 21CS30011\n",
    "\n",
    "\\\n",
    "\\\n",
    "\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries here : \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset and analysing the type of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 13)\n",
      "    Area  Perimeter  Major_Axis_Length  Minor_Axis_Length  Convex_Area  \\\n",
      "0  56276    888.242           326.1485           220.2388        56831   \n",
      "\n",
      "   Equiv_Diameter  Eccentricity  Solidity  Extent  Roundness  Aspect_Ration  \\\n",
      "0        267.6805        0.7376    0.9902  0.7453     0.8963         1.4809   \n",
      "\n",
      "   Compactness       Class  \n",
      "0       0.8207  Çerçevelik  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('../../dataset/logistic-regression/Pumpkin_Seeds_Dataset.xlsx')\n",
    "print(df.shape) \n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PreProcessing the Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.60882926  0.6459801  ... -0.30180065  0.01488933\n",
      "  -0.14217799]\n",
      " [ 1.          0.32818862 -0.24722132 ...  1.58513179 -1.47631136\n",
      "   1.68881593]\n",
      " [ 1.          1.2310118   1.65276095 ... -1.43895861  1.61791432\n",
      "  -1.50887079]\n",
      " ...\n",
      " [ 1.          1.01272767  0.49924928 ...  0.97281596 -0.95309963\n",
      "   0.98300334]\n",
      " [ 1.          0.07163201 -0.07652727 ...  0.42476652 -0.42705291\n",
      "   0.34458931]\n",
      " [ 1.         -0.105833   -0.10490407 ...  0.06773106 -0.05661523\n",
      "  -0.03920798]]\n",
      "[1 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# PreProcessing the data : \n",
    "\n",
    "# Encode the 'Class' column to numerical values\n",
    "class_mapping = {'Çerçevelik': 0, 'Ürgüp Sivrisi': 1}\n",
    "df['Class'] = df['Class'].map(class_mapping)\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_data, temp_data = train_test_split(df, test_size=0.5, random_state=42)\n",
    "valid_data, test_data = train_test_split(temp_data, test_size=0.4, random_state=42)\n",
    "\n",
    "# Separate features and target variables\n",
    "X_train = train_data.drop('Class', axis=1).values\n",
    "y_train = train_data['Class'].values\n",
    "\n",
    "X_valid = valid_data.drop('Class', axis=1).values\n",
    "y_valid = valid_data['Class'].values\n",
    "\n",
    "X_test = test_data.drop('Class', axis=1).values\n",
    "y_test = test_data['Class'].values\n",
    "\n",
    "# Normalize/Standardize the features\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "X_train = (X_train - mean) / std\n",
    "X_valid = (X_valid - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "# Add a column of 1's (bias term) to X_train, X_valid, and X_test using NumPy\n",
    "bias_column_train = np.ones((X_train.shape[0], 1))\n",
    "\n",
    "\n",
    "# Concatenate the bias column with the original X_train, X_valid, and X_test\n",
    "X_train = np.hstack((bias_column_train, X_train))\n",
    "X_valid = np.hstack((np.ones((X_valid.shape[0], 1)), X_valid))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model :\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict(X, weights):\n",
    "    return sigmoid(np.dot(X,weights.T))\n",
    "\n",
    "def logistic_regression(X, y, num_epochs, learning_rate):\n",
    "    num_samples, num_features = X.shape\n",
    "    weights = np.zeros(num_features)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        y_pred = predict(X, weights)\n",
    "        gradient = np.dot(X.T, (y_pred - y)) / num_samples\n",
    "        weights -= learning_rate * gradient\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting values for test data based :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training the logistic regression model\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "weights = logistic_regression(X_train, y_train, num_epochs, learning_rate)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_valid = predict(X_valid, weights)\n",
    "y_pred_valid_class = np.round(y_pred_valid)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = predict(X_test, weights)\n",
    "y_pred_test_class = np.round(y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating how well the model is performing on training and test data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Metrics:\n",
      "Accuracy: 0.84533\n",
      "Precision: 0.85797\n",
      "Recall: 0.81543\n",
      "\n",
      "Test Set Metrics:\n",
      "Accuracy: 0.85400\n",
      "Precision: 0.83750\n",
      "Recall: 0.85532\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# Evaluate on validation set\n",
    "accuracy_valid, precision_valid, recall_valid = evaluate_metrics(y_valid, y_pred_valid_class)\n",
    "\n",
    "# Evaluate on test set\n",
    "accuracy_test, precision_test, recall_test = evaluate_metrics(y_test, y_pred_test_class)\n",
    "\n",
    "print(\"Validation Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_valid:.5f}\")\n",
    "print(f\"Precision: {precision_valid:.5f}\")\n",
    "print(f\"Recall: {recall_valid:.5f}\")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_test:.5f}\")\n",
    "print(f\"Precision: {precision_test:.5f}\")\n",
    "print(f\"Recall: {recall_test:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
